


########################################
## 1) Load Libraries
########################################
library(dplyr)
library(tidyr)
library(challengeR)
library(readr)

########################################
## 2) Get all model directories
########################################
parent_dir <- "./totalsegmentator_results"
model_dirs <- list.dirs(parent_dir, recursive = FALSE)

# Define the organs you expect (should match the column names in each dsc.csv)
organs <- c("aorta", "gall_bladder", "kidney_left", "kidney_right",
            "liver", "pancreas", "postcava", "spleen", "stomach")

########################################
## 3) Read and combine DSC.csv files from all models
########################################
all_data_long <- data.frame()

for (mdir in model_dirs) {
  csv_file <- file.path(mdir, "dsc.csv")

  if (!file.exists(csv_file)) {
    message("Skipping ", mdir, ": dsc.csv not found.")
    next
  }

  # Read the CSV file (each file should have a 'name' column and columns for organs)
  df_wide <- read_csv(csv_file, show_col_types = FALSE)

  if (!"name" %in% names(df_wide)) {
    message("Skipping ", csv_file, ": 'name' column not found.")
    next
  }

  # Pivot the data from wide to long format: each row = one case & one organ
  df_long <- df_wide %>%
    pivot_longer(
      cols = -name,
      names_to = "organ",
      values_to = "dsc"
    )

  # Keep only the organs of interest
  df_long <- df_long %>%
    filter(organ %in% organs)

  # Add a column for the algorithm name (using the directory name)
  df_long <- df_long %>%
    mutate(Algorithm = basename(mdir))

  # Append this model's data to the master data frame
  all_data_long <- bind_rows(all_data_long, df_long)
}

########################################
## 4) Create a challenge object
########################################
# Each row is a case for a particular organ (task) and algorithm
my_challenge <- as.challenge(
  object      = all_data_long,
  case        = "name",
  algorithm   = "Algorithm",
  value       = "dsc",
  by          = "organ",    # Each organ is treated as a separate task
  smallBetter = FALSE,      # DSC: higher is better
  na.treat    = "na.rm",    # Remove rows with NA automatically
  check       = TRUE
)

########################################
## 5) Compute aggregated metrics (mean and standard deviation)
########################################
mean_obj <- aggregate(my_challenge, FUN = "mean", na.treat = "na.rm")
sd_obj   <- aggregate(my_challenge, FUN = "sd",   na.treat = "na.rm")

# Get the list of organs for which results are available
organs_present <- names(mean_obj$matlist)

########################################
## 6) Build a combined result table with "mean ± sd"
########################################
res_table <- data.frame()

for (org in organs_present) {
  df_mean <- mean_obj$matlist[[org]]
  df_sd   <- sd_obj$matlist[[org]]

  # The aggregate function places the computed value in the last column
  col_mean <- colnames(df_mean)[ncol(df_mean)]
  col_sd   <- colnames(df_sd)[ncol(df_sd)]

  tmp <- data.frame(
    Algorithm = rownames(df_mean),
    organ     = org,
    DSC_mean  = df_mean[[col_mean]],
    DSC_sd    = df_sd[[col_sd]]
  )

  # Format as "mean ± sd" (multiplying by 100 to express as percentage)
  tmp$DSC_string <- sprintf("%.1f ± %.1f", 100 * tmp$DSC_mean, 100 * tmp$DSC_sd)

  res_table <- rbind(res_table, tmp)
}

# Reshape to a wide table: one row per Algorithm, one column per organ
final_table <- res_table %>%
  select(Algorithm, organ, DSC_string) %>%
  pivot_wider(
    names_from  = organ,
    values_from = DSC_string
  ) %>%
  arrange(Algorithm)

########################################
## 7) Print the full table (all rows & columns)
########################################
print(final_table, n = Inf, width = Inf)



# ---------------------------------------------------
# 0) Packages
# ---------------------------------------------------
library(challengeR)
library(dplyr)
library(purrr)
library(readr)
library(ggplot2)

# ---------------------------------------------------
# 1) Set up
# ---------------------------------------------------
parent_dir <- "./totalsegmentator_results"
model_dirs <- list.dirs(parent_dir, recursive = FALSE)

# The organs you have columns for:
organs <- c("aorta", "gall_bladder", "kidney_left", "kidney_right",
            "liver", "pancreas", "postcava", "spleen", "stomach")

# We'll store the final boxplots in a list
boxplots <- list()

# ---------------------------------------------------
# 2) For each organ, gather data from all model dirs
# ---------------------------------------------------
for (org in organs) {

  message("Processing organ: ", org)

  # Read from all model dirs & keep only 'name' + that organ
  df_all <- map_dfr(model_dirs, function(mdir) {
    csv_file <- file.path(mdir, "dsc.csv")
    if (!file.exists(csv_file)) {
      message("Skipping: no dsc.csv in ", mdir)
      return(NULL)
    }
    tmp <- read_csv(csv_file, show_col_types = FALSE)

    # Each CSV has 'name', plus columns for the 9 organs
    # We'll keep 'name' and the current 'org' column
    # But check that 'org' column exists
    if (!org %in% names(tmp)) {
      message("Skipping: column ", org, " not found in ", csv_file)
      return(NULL)
    }

    tmp <- tmp %>%
      select(name, all_of(org))

    # Rename columns to fit as.challenge()
    # 'case' => name
    # 'value' => the organ column
    tmp <- tmp %>%
      rename(case = name, value = !!org) %>%
      mutate(algo = basename(mdir))   # model name from subdir
  })

  # If no data was read, skip
  if (nrow(df_all) == 0) {
    message("No data for organ ", org, " - skipping.")
    next
  }

  # ---------------------------------------------------
  # 3) Construct challenge & compute ranking
  # ---------------------------------------------------
  # Single-task challenge for this organ
  chal <- as.challenge(
    df_all,
    algorithm = "algo",
    case      = "case",
    value     = "value",
    taskName  = org,        # label the task as the organ
    smallBetter = TRUE,     # Reverse ordering here
    na.treat    = "na.rm"    # remove rows with NA
  )

  ranking <- chal %>%
    aggregateThenRank(FUN = median, ties.method = "min")

  # ---------------------------------------------------
  # 4) Create boxplot for this organ, with algo on Y
  # ---------------------------------------------------
  # By default, challengeR::boxplot() puts algo on X-axis,
  # so we can just add + coord_flip() to swap.
  p <- boxplot(ranking) +
       coord_flip() +
       ggtitle(paste("Organ:", org)) +
       xlab("DSC") +
       ylab("Algorithm")

  # Store in a list
  boxplots[[org]] <- p
}

# ---------------------------------------------------
# 5) Inspect or save the resulting list of plots
# ---------------------------------------------------
# For example, print them in console:
# boxplots[["liver"]]
# boxplots[["kidney_left"]]
# etc.

# Optionally, you could loop & save each to PDF
# for (org in names(boxplots)) {
#   ggsave(filename=paste0(org,"_boxplot.pdf"), boxplots[[org]],
#          width=6, height=4)
# }


library(patchwork)   # if not installed: install.packages("patchwork")

combined_plot <- wrap_plots(boxplots, ncol = 3)
print(combined_plot)


for (p in boxplots) {
  print(p)
  Sys.sleep(1)  # optional: pause between plots
}






# ---------------------------------------------------
# 0) Packages
# ---------------------------------------------------
library(challengeR)
library(dplyr)
library(purrr)
library(readr)
library(ggplot2)

# ---------------------------------------------------
# 1) Set up
# ---------------------------------------------------
parent_dir <- "./totalsegmentator_results"
model_dirs <- list.dirs(parent_dir, recursive = FALSE)

# The 9 organs you have columns for:
organs <- c("aorta", "gall_bladder", "kidney_left", "kidney_right",
            "liver", "pancreas", "postcava", "spleen", "stomach")

# We'll store the final boxplots in a list
boxplots <- list()

# ---------------------------------------------------
# 2) For each organ, gather data from all model dirs
# ---------------------------------------------------
for (org in organs) {
  message("Processing organ: ", org)

  # Gather CSV from each model directory
  df_all <- map_dfr(model_dirs, function(mdir) {
    csv_file <- file.path(mdir, "dsc.csv")
    if (!file.exists(csv_file)) {
      message("Skipping: no dsc.csv in ", mdir)
      return(NULL)
    }
    tmp <- read_csv(csv_file, show_col_types = FALSE)

    if (!org %in% names(tmp)) {
      message("Skipping: column ", org, " not found in ", csv_file)
      return(NULL)
    }

    # Keep only 'name' and the column for this organ
    tmp <- tmp %>%
      select(name, all_of(org)) %>%
      rename(case = name, value = !!org) %>%
      mutate(algo = basename(mdir))
  })

  # If no data was read, skip
  if (nrow(df_all) == 0) {
    message("No data for organ ", org, " - skipping.")
    next
  }

  # ---------------------------------------------------
  # 3) Construct challenge & compute ranking
  # ---------------------------------------------------
  # Single-task challenge for this organ
  chal <- as.challenge(
    df_all,
    algorithm   = "algo",
    case        = "case",
    value       = "value",
    taskName    = org,       
    smallBetter = FALSE,     # lower rank is better
    na.treat    = "na.rm"   
  )

  ranking <- chal %>%
    aggregateThenRank(FUN = median, ties.method = "min")

  # ---------------------------------------------------
  # 4) Restrict to top 5 algorithms
  # ---------------------------------------------------
  # subset(..., top=5) returns a new "ranking" object 
  # containing only the top 5 (or more, if ties).
  ranking_top5 <- subset(ranking, top = 5)

  # ---------------------------------------------------
  # 5) Create boxplot for these top 5 algorithms
  # ---------------------------------------------------
  # 'boxplot(ranking_top5)' will automatically show only 
  # the included algorithms. Then we do coord_flip().
  p <- boxplot(ranking_top5) +
       coord_flip() +
       ggtitle(paste("Organ:", org, "(Top 5)")) +
       xlab("DSC") +
       ylab("Algorithm")

  # Store the plot in our list
  boxplots[[org]] <- p
}

# ---------------------------------------------------
# 6) Visualize the resulting plots individually
# ---------------------------------------------------
# Instead of combining them into a grid (which makes them tiny),
# print each plot one at a time.
for (p in boxplots) {
  print(p)
  Sys.sleep(1)  # Optional: pause between plots; adjust or remove as needed.
}



# ?aggregateThenRank





# ---------------------------------------------------
# 0) Packages
# ---------------------------------------------------
library(challengeR)
library(dplyr)
library(purrr)
library(readr)
library(ggplot2)

# ---------------------------------------------------
# 1) Set up
# ---------------------------------------------------
parent_dir <- "./totalsegmentator_results"
model_dirs <- list.dirs(parent_dir, recursive = FALSE)

# The 9 organs you have columns for:
organs <- c("aorta", "gall_bladder", "kidney_left", "kidney_right",
            "liver", "pancreas", "postcava", "spleen", "stomach")

# We'll store the final violin plots in a list
violinplots <- list()

# ---------------------------------------------------
# 2) For each organ, gather data and produce violin plot
# ---------------------------------------------------
for (org in organs) {
  message("Processing organ: ", org)

  # Gather CSV from each model directory
  df_all_for_this_organ <- map_dfr(model_dirs, function(mdir) {
    csv_file <- file.path(mdir, "dsc.csv")
    if (!file.exists(csv_file)) {
      message("Skipping: no dsc.csv in ", mdir)
      return(NULL)
    }
    tmp <- read_csv(csv_file, show_col_types = FALSE)

    if (!org %in% names(tmp)) {
      message("Skipping: column ", org, " not found in ", csv_file)
      return(NULL)
    }

    tmp <- tmp %>%
      select(name, all_of(org)) %>%
      rename(case = name, value = !!org) %>%
      mutate(algo = basename(mdir))
  })

  # If no data was read, skip
  if (nrow(df_all_for_this_organ) == 0) {
    message("No data for organ ", org, " - skipping.")
    next
  }

  # ---------------------------------------------------
  # 3) Construct challenge & compute ranking
  # ---------------------------------------------------
  chal <- as.challenge(
    df_all_for_this_organ,
    algorithm   = "algo",
    case        = "case",
    value       = "value",
    taskName    = org,       
    smallBetter = FALSE,  # or TRUE if you want smaller DSC => better
    na.treat    = "na.rm"
  )

  ranking <- chal %>%
    aggregateThenRank(FUN = median, ties.method = "min")

  # ---------------------------------------------------
  # 4) Restrict to top 5 algorithms
  # ---------------------------------------------------
  ranking_top5 <- subset(ranking, top = 5)

  # ---------------------------------------------------
  # 5) Bootstrap the top-5 ranking
  # ---------------------------------------------------
  # 'violin()' requires a *bootstrapped* ranking.
  rankingBoot <- ranking_top5 %>%
    bootstrap(nboot = 500, parallel = FALSE, progress = "none")

  # ---------------------------------------------------
  # 6) Create violin plot
  # ---------------------------------------------------
  # This is a violin of rank distributions, not DSC values.
  p <- violin(rankingBoot) +
       coord_flip() +
       ggtitle(paste("Violin Plot:", org, "(Top 5)")) +
       xlab("Algorithm") +
       ylab("Bootstrap distribution of ranks")

  # Store the plot in our list
  violinplots[[org]] <- p
}

# ---------------------------------------------------
# 7) Visualize the resulting violin plots individually
# ---------------------------------------------------
for (p in violinplots) {
  print(p)
  Sys.sleep(1)  # Optional: pause between plots
}






library(challengeR)
library(dplyr)
library(purrr)
library(readr)
library(ggplot2)

# ---------------------------------------------------
# 0) Parameters
# ---------------------------------------------------
parent_dir <- "./totalsegmentator_results/final_14dec2024_results/totalsegmentator_results - table 3 in paper"
model_dirs <- list.dirs(parent_dir, recursive = FALSE)

organs <- c("aorta", "gall_bladder", "kidney_left", "kidney_right",
            "liver", "pancreas", "postcava", "spleen", "stomach")

# We'll gather everything into one big data frame:
# columns => case, algo, value, task(=organ)
df_big <- NULL

# ---------------------------------------------------
# 1) Loop over each model directory and read DSC
# ---------------------------------------------------
for (mdir in model_dirs) {
  csv_file <- file.path(mdir, "dsc.csv")
  if (!file.exists(csv_file)) {
    message("Skipping: no dsc.csv in ", mdir)
    next
  }
  
  tmp_whole <- read_csv(csv_file, show_col_types = FALSE)
  
  # For each organ, we pivot the data so we get (case, algo, value, task)
  # We'll gather all 9 organs from the same CSV in "long" format
  long_part <- tmp_whole %>%
    select(name, all_of(organs)) %>%   # keep name plus the organ columns
    tidyr::pivot_longer(
      cols      = all_of(organs),
      names_to  = "task",     # "task" is the organ name
      values_to = "value"
    ) %>%
    rename(case = name) %>%
    mutate(algo = basename(mdir))  # which model?
  
  df_big <- bind_rows(df_big, long_part)
}

# Filter out rows where we have no DSC (NA)
df_big <- df_big %>%
  filter(!is.na(value))

# Now df_big has columns: case, task (organ), value, algo
head(df_big)



chal <- as.challenge(
  df_big,
  algorithm   = "algo",   # which column says which model?
  case        = "case",   # which column has the patient ID?
  value       = "value",  # DSC scores
  by          = "task",   # treat each organ as a separate "task"
  smallBetter = FALSE,    # DSC => bigger is better
  na.treat    = "na.rm"   # in case some partial data exist
)


ranking <- chal %>%
  testThenRank(
    alpha           = 0.05,   # significance level
    p.adjust.method = "none", # how to correct for multiple comparisons
    ties.method     = "min"
    # by default, Wilcoxon is used, smallBetter=FALSE => one-sided test
  )


# significanceMap returns either a ggplot or a list of ggplots.
sigplots <- significanceMap(ranking)

# If you have only 1 or 2 tasks, it might return a single ggplot object.
# But for 9 tasks, it returns a list of 9 ggplot objects, each titled by the organ name.
# Let's print them all:
if (inherits(sigplots, "ggplot")) {
  print(sigplots)
} else {
  # It's a list of ggplots, so we can loop:
  for (oneplot in sigplots) {
    print(oneplot)
    Sys.sleep(1)
  }
}



plots_solo <- list()
for (org in organs) {
  # Filter to that organ only:
  sub_df <- df_big %>% filter(task == org)
  if (nrow(sub_df) == 0) next
  
  chal_solo <- as.challenge(
    sub_df,
    algorithm   = "algo",
    case        = "case",
    value       = "value",
    taskName    = org,
    smallBetter = FALSE,
    na.treat    = "na.rm"
  )
  
  ranking_solo <- chal_solo %>%
    testThenRank(alpha=0.05, ties.method="min")
  
  pmap <- significanceMap(ranking_solo) +
    ggtitle(paste("Significance Map:", org))
  
  plots_solo[[org]] <- pmap
}

# Print them
for (pp in plots_solo) {
  print(pp)
  Sys.sleep(1)
}






library(challengeR)
library(dplyr)
library(tidyr)
library(readr)

parent_dir <- "./totalsegmentator_results"  # e.g. your path
model_dirs <- list.dirs(parent_dir, recursive = FALSE)

organs <- c("aorta","gall_bladder","kidney_left","kidney_right",
            "liver","pancreas","postcava","spleen","stomach")

df_big <- NULL

for (mdir in model_dirs) {
  csv_file <- file.path(mdir, "dsc.csv")
  if (!file.exists(csv_file)) {
    message("Skipping no dsc.csv in ", mdir)
    next
  }
  tmp_whole <- read_csv(csv_file, show_col_types = FALSE)

  # pivot: ( name, aorta, spleen, etc ) -> long format => (name, task, value)
  long_part <- tmp_whole %>%
    select(name, all_of(organs)) %>%
    pivot_longer(
      cols      = organs,
      names_to  = "task",    # "task" is your organ
      values_to = "value"
    ) %>%
    rename(case = name) %>%
    mutate(algo = basename(mdir))

  df_big <- bind_rows(df_big, long_part)
}

# Filter out NA
df_big <- df_big %>% filter(!is.na(value))

# Now df_big has columns: (case, task, value, algo)
head(df_big)


# A) Single aggregated significance map

library(ggplot2)

# Create a new column that uniquely identifies each (case, task) combination
df_big2 <- df_big %>%
  mutate(superCase = paste(case, task, sep = "_"))  # <--- fix the column name here

chal_agg <- as.challenge(
  df_big2,
  algorithm   = "algo",
  case        = "superCase",  # each row is a unique (case, organ)
  value       = "value",
  smallBetter = FALSE,
  check       = FALSE
)

# IMPORTANT: When calling testThenRank, supply alpha, case, and smallBetter!
ranking_agg <- chal_agg %>%
  testThenRank(
    alpha = 0.05,
    case = "superCase",
    smallBetter = FALSE,
    ties.method = "min"
  )

# Create a single heatmap (since there is effectively only 1 "task" now)
sigmap <- significanceMap(ranking_agg)

print(sigmap)






library(dplyr)
library(tidyr)
library(pheatmap)

# 1) We'll just use df_big2 directly. The relevant columns are:
#    - df_big2$superCase   (unique combination of (case,organ))
#    - df_big2$algo
#    - df_big2$value

df_for_custom <- df_big2 %>%
  select(superCase, algo, value)

# 2) Identify all algorithms
all_algos <- sort(unique(df_for_custom$algo))

# 3) Create an empty matrix for p-values
pmat <- matrix(NA, 
               nrow = length(all_algos), 
               ncol = length(all_algos),
               dimnames = list(all_algos, all_algos))

# 4) Fill pmat with the paired Wilcoxon p-value, 
#    "alternative='greater'" if bigger DSC => better
for (i in seq_along(all_algos)) {
  for (j in seq_along(all_algos)) {
    if (i == j) {
      pmat[i, j] <- NA
    } else if (j < i) {
      # skip lower triangle or fill symmetrically
      next
    } else {
      algoA <- all_algos[i]
      algoB <- all_algos[j]

      dfA <- df_for_custom %>% filter(algo == algoA)
      dfB <- df_for_custom %>% filter(algo == algoB)

      merged <- dfA %>%
        rename(valA = value) %>%
        inner_join(
          dfB %>% rename(valB = value),
          by = "superCase"
        )
      # now 'merged' has columns: superCase, valA, algo.x, valB, algo.y

      # if no overlapping superCase => can't do test => p=NA
      if (nrow(merged) == 0) {
        pmat[i, j] <- NA
        pmat[j, i] <- NA
        next
      }

      wout <- wilcox.test(
        merged$valA, merged$valB,
        alternative = "greater",  # bigger DSC => better
        paired = TRUE, exact = FALSE
      )
      pmat[i, j] <- wout$p.value
      pmat[j, i] <- wout$p.value  # symmetrical
    }
  }
}

# 5) Visualize with a color scale from 0->1 (or saturate at 0.05)
my_colors <- colorRampPalette(c("yellow", "white", "blue"))(100)

pheatmap(
  pmat,
  color          = my_colors,
  cluster_rows   = FALSE,
  cluster_cols   = FALSE,
  display_numbers= TRUE,
  number_format  = "%.3g",
  main           = "Paired Wilcoxon p-values (A > B)",
  cellwidth      = 25,   # increase cell width
  cellheight     = 25,   # increase cell height
  fontsize       = 12    # adjust font size if needed
)




png("heatmap.png", width = 2600, height = 2200)  # Increase width and height as needed
pheatmap(
  pmat,
  color          = my_colors,
  cluster_rows   = FALSE,
  cluster_cols   = FALSE,
  display_numbers= TRUE,
  number_format  = "%.3g",
  main           = "Paired Wilcoxon p-values (A > B)",
  cellwidth      = 30,
  cellheight     = 30,
  fontsize       = 12
)
dev.off()



library(dplyr)
library(tidyr)
library(ggplot2)

## 1) We assume you have a data frame “df_for_custom” with columns:
##      superCase, algo, value
##   i.e. each row is a specific (case+organ), an algorithm, and the DSC value.

# For illustration, suppose you already built df_for_custom from df_big:
#   df_big2 = df_big %>% mutate(superCase = paste(case, task, sep="_"))
#   df_for_custom = df_big2 %>% select(superCase, algo, value)

######## 2) Identify the algorithms ########
all_algos <- sort(unique(df_for_custom$algo))

######## 3) Compute p-value matrix pmat ########
pmat <- matrix(NA,
               nrow = length(all_algos), ncol = length(all_algos),
               dimnames = list(all_algos, all_algos))

for (i in seq_along(all_algos)) {
  for (j in seq_along(all_algos)) {
    if (i == j) {
      pmat[i, j] <- NA
    } else if (j < i) {
      # skip or fill symmetrically
      next
    } else {
      algoA <- all_algos[i]
      algoB <- all_algos[j]

      # gather rows for algoA and algoB
      dfA <- df_for_custom %>% filter(algo == algoA)
      dfB <- df_for_custom %>% filter(algo == algoB)

      # merge by superCase
      merged <- dfA %>%
        rename(valA = value) %>%
        inner_join(dfB %>% rename(valB = value), by = "superCase")

      if (nrow(merged) == 0) {
        # no overlapping superCase
        pmat[i, j] <- NA
        pmat[j, i] <- NA
        next
      }

      # Wilcoxon one-sided test (bigger = better)
      wout <- wilcox.test(merged$valA, merged$valB,
                          alternative = "greater",
                          paired = TRUE, exact = FALSE)
      pval <- wout$p.value
      pmat[i, j] <- pval
      pmat[j, i] <- pval
    }
  }
}

######## 4) Convert p-values to categories ########

# For example, define a function that cuts p-values into confidence tiers:
cutConfidence <- function(p) {
  if (is.na(p)) return(NA_character_)
  # "p < 0.001 => >=99.9% confidence"
  else if (p < 0.00001) return("≥99.999%")
  else if (p < 0.0001) return("≥99.99%")
  else if (p < 0.001) return("≥99.9%")
  else if (p < 0.01)  return("≥99%")
  else if (p < 0.05)  return("≥95%")
  else if (p < 0.10)  return("≥90%")
  else if (p < 0.25)  return("≥75%")
  else               return("<75%")
}

# We'll make a data frame with row, col, pval, and category
df_heat <- expand.grid(
  algoRow = all_algos,
  algoCol = all_algos,
  stringsAsFactors = FALSE
)

df_heat$pval <- mapply(
  function(r, c) pmat[r, c],
  df_heat$algoRow,
  df_heat$algoCol
)

df_heat$confidenceTier <- sapply(df_heat$pval, cutConfidence)

# For convenience, we can define an ordering of these categories:
tier_levels <- c("≥99.999%", "≥99.99%", "≥99.9%", "≥99%", "≥95%", "≥90%", "≥75%", "<75%")
df_heat$confidenceTier <- factor(df_heat$confidenceTier, levels = tier_levels)

######## 5) Plot in ggplot2 ########

# We'll do a tile-based heatmap:
#   color/fill = the confidenceTier factor
ggplot(df_heat, aes(x = algoRow, y = algoCol, fill = confidenceTier)) +
  geom_tile(color = "grey40") +
  # Optionally, put text in each tile => the tier name or p-value
  geom_text(aes(label = confidenceTier), color = "black", size = 3) +
  scale_fill_brewer(palette = "RdYlBu", na.value = "white", drop = FALSE) +
  coord_fixed() +
  theme_minimal(base_size = 13) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
        panel.grid = element_blank()) +
  labs(title = "Discrete Confidence Tier Heatmap",
       subtitle = "Wilcoxon A> B test, bigger=better",
       x = "Algorithm (Row)", y = "Algorithm (Column)",
       fill = "Confidence Tier")



write.csv(df_heat, "df_heat.csv", row.names = FALSE)


library(dplyr)
library(tidyr)
library(ggplot2)
library(pheatmap)  # for reference; we'll build our own ggplot

# Suppose you already have a data frame 'df_for_custom'
# with columns: superCase, algo, and value.
# (This is obtained after merging organ and case info.)

# 1) Identify unique algorithms.
all_algos <- sort(unique(df_for_custom$algo))

# 2) Create an empty matrix for p-values.
pmat <- matrix(NA, nrow = length(all_algos), ncol = length(all_algos),
               dimnames = list(all_algos, all_algos))

# 3) Fill the matrix using paired Wilcoxon tests.
for (i in seq_along(all_algos)) {
  for (j in seq_along(all_algos)) {
    if (i == j) {
      pmat[i, j] <- NA  # Self-comparison: leave as NA
    } else if (j < i) {
      # Fill symmetrically
      next
    } else {
      algoA <- all_algos[i]
      algoB <- all_algos[j]
      
      dfA <- df_for_custom %>% filter(algo == algoA)
      dfB <- df_for_custom %>% filter(algo == algoB)
      
      # Merge by superCase to align values.
      merged <- dfA %>%
        rename(valA = value) %>%
        inner_join(dfB %>% rename(valB = value), by = "superCase")
      
      if (nrow(merged) == 0) {
        pmat[i, j] <- NA
        pmat[j, i] <- NA
        next
      }
      
      # Wilcoxon paired test (alternative = "greater" means we test if A > B).
      test_out <- wilcox.test(merged$valA, merged$valB, alternative = "greater",
                              paired = TRUE, exact = FALSE)
      pval <- test_out$p.value
      pmat[i, j] <- pval
      pmat[j, i] <- pval  # Keep the matrix symmetric.
    }
  }
}

# 4) Convert continuous p-values to discrete confidence tiers.
cutConfidence <- function(p) {
  if (is.na(p)) return(NA_character_)
  # "p < 0.001 => >=99.9% confidence"
  else if (p < 0.00001) return("≥99.999%")
  else if (p < 0.0001) return("≥99.99%")
  else if (p < 0.001) return("≥99.9%")
  else if (p < 0.01)  return("≥99%")
  else if (p < 0.05)  return("≥95%")
  else if (p < 0.10)  return("≥90%")
  else if (p < 0.25)  return("≥75%")
  else               return("<75%")
}

df_heat <- expand.grid(
  algoRow = all_algos,
  algoCol = all_algos,
  stringsAsFactors = FALSE
)

df_heat$pval <- mapply(function(r, c) pmat[r, c],
                       df_heat$algoRow,
                       df_heat$algoCol)

df_heat$confidenceTier <- sapply(df_heat$pval, cutConfidence)
tier_levels <- c("≥99.999%", "≥99.99%", "≥99.9%", "≥99%", "≥95%", "≥90%", "≥75%", "<75%")
df_heat$confidenceTier <- factor(df_heat$confidenceTier, levels = tier_levels)

# 5) Create a ggplot heatmap.
# You can try different palettes here:
# - "RdYlBu" from RColorBrewer is one option.
# - Other options: "Spectral", "PuOr", "YlGnBu", or even using the viridis package.




heatmap_plot <- ggplot(df_heat, aes(x = algoRow, y = algoCol, fill = confidenceTier)) +
  geom_tile(color = "grey40") +
  geom_text(aes(label = confidenceTier), color = "black", size = 3) +
  scale_fill_brewer(palette = "hawaii", na.value = "white", drop = FALSE) +
  coord_fixed() +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
        panel.grid = element_blank()) +
  labs(title = "Paired Wilcoxon Confidence Tier Heatmap",
       subtitle = "Confidence levels for A > B",
       x = "Algorithm (Row)",
       y = "Algorithm (Column)",
       fill = "Confidence Tier")

# 6) Save the plot to file with large dimensions.
ggsave("confidence_heatmap.png", heatmap_plot, width = 12, height = 10, dpi = 300)








# Install ChallengeR from github

significance <- challengeR:::significance

testThenRank <- function(object, ties.method = "min", ...) {
  object %>% aggregate(FUN = significance, ...) %>% rank(ties.method = ties.method)
}






##############################################################################
## 0) Libraries and Setup
##############################################################################
library(challengeR)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(purrr)      # for map()
library(gridExtra)  # optionally to arrange multiple plots

# Path to parent folder containing one subfolder per algorithm
parent_dir <- "./totalsegmentator_results/final_14dec2024_results/totalsegmentator_results - table 3 in paper"

# List subdirectories (one per algorithm)
model_dirs <- list.dirs(parent_dir, recursive = FALSE)

# Our 9 organs of interest
all_organs <- c("aorta","gall_bladder","kidney_left","kidney_right",
                "liver","pancreas","postcava","spleen","stomach")

# Decide which metric to use for each organ: DSC vs NSD
# Example: aorta & postcava => NSD; everything else => DSC
organs_use_nsd <- c("aorta","postcava")
organs_use_dsc <- setdiff(all_organs, organs_use_nsd)


cat("\n--- head(chal_org, 10) ---\n")
print(head(chal_org, 10))

cat("\nNumber of distinct algos:\n")
print(length(unique(chal_org[["algo"]])))

cat("\nNumber of distinct cases:\n")
print(length(unique(chal_org[["case"]])))

cat("\nAny duplicates in (algo, case)?\n")
dupCount <- sum(duplicated(chal_org[, c("algo","case")]))
print(dupCount)   # Expect 0



chal_org


write.csv(df_metric, "organ_perf_dsc_merged.csv", row.names = FALSE)





# View unique organ categories
unique(df_metric$algo)

# Count occurrences of each organ category
table(df_metric$algo)



chal_org %>% 
  aggregateThenRank(FUN=mean, na.treat="na.rm", ties.method="min")



##############################################################################
## 1) Utility function: read CSV from each algorithm folder, pivot to long
##############################################################################
# We'll define a helper that:
# - Takes e.g. metric="dsc" or "nsd"
# - Returns a big data.frame with columns: (case, organ, value, algo)
#   for all algorithms that have that CSV.
read_metric_data <- function(metric = c("dsc","nsd"), organs) {
  metric <- match.arg(metric)  # ensure it's "dsc" or "nsd"
  
  df_out <- map_dfr(model_dirs, function(mdir) {
    csv_file <- file.path(mdir, paste0(metric, ".csv"))
    if (!file.exists(csv_file)) {
      message("Skipping: no ", metric, ".csv in ", mdir)
      return(NULL)
    }
    # read the CSV
    tmp <- read_csv(csv_file, show_col_types = FALSE)
    # pivot longer for the organs we want
    # we rename "name" -> "case"
    # final columns: (case, organ, value), plus the "algo" from folder name
    long_part <- tmp %>%
      select(name, all_of(organs)) %>%
      pivot_longer(
        cols      = all_of(organs),
        names_to  = "organ",
        values_to = "value"
      ) %>%
      rename(case = name) %>%
      mutate(algo = basename(mdir))
    
    # remove rows where value is NA
    long_part <- filter(long_part, !is.na(value))
    long_part
  })
  
  return(df_out)
}




##############################################################################
## 2) We'll do an organ-by-organ loop. 
##    For each organ, gather the data from the correct metric CSV, 
##    build challenge, do testThenRank, produce 3 plots.
##############################################################################

# We store results (tables/plots) in a list
results_per_organ <- list()

# Directory to save plots (optional)
out_plots_dir <- "organ_plots"
dir.create(out_plots_dir, showWarnings = FALSE)

for (org in all_organs) {
  # 2a) Decide which metric for this organ
  if (org %in% organs_use_nsd) {
    metric_choice <- "nsd"
  } else {
    metric_choice <- "dsc"
  }
  
  # 2b) Read the data for *just* that organ from the chosen metric
  #     We can read the entire CSV pivot for only that one organ:
  df_metric <- read_metric_data(metric = metric_choice, organs = c(org))
  # Now df_metric has columns: (case, organ, value, algo)
  
  if (nrow(df_metric) == 0) {
    message("No data found for organ ", org, " with metric=", metric_choice," -- skipping.")
    next
  }
  
  # 2c) Build challenge object
  # smallBetter=FALSE if bigger metric => better
  # skip checks if you want: check=FALSE
  chal_org <- as.challenge(
    df_metric,
    algorithm   = "algo",
    case        = "case",
    value       = "value",
    smallBetter = FALSE,
    check       = FALSE
  )
  
  # 2d) Compute ranking with testThenRank
  rank_org <- chal_org %>%
    testThenRank(alpha = 0.05, ties.method = "min")
  
  # 2e) (Optional) do bootstrapping for a violin plot of ranks
  #     If you have enough data (cases). 
  #     If it's small, you can skip or reduce nboot.
  ranking_boot <- rank_org %>%
    bootstrap(nboot = 200, parallel = FALSE, progress = "none")
  
  ############################################################################
  ## 3) Generate 3 example figures:
  ##    1) Boxplot of raw metric across algorithms
  ##    2) Significance map from challengeR
  ##    3) Violin of rank distributions from the bootstrapped ranking
  ############################################################################
  
  ### Figure 1: boxplot of raw metric
  # We'll do a simple ggplot box + jitter for df_metric
  p_box <- ggplot(df_metric, aes(x = algo, y = value, color = algo)) +
    geom_boxplot(outlier.shape = NA) +
    geom_jitter(width = 0.2, alpha = 0.6, size = 1.5) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
    labs(title = paste("Boxplot for", org, "using", toupper(metric_choice)),
         x = "Algorithm", y = toupper(metric_choice))
  
  ### Figure 2: significance map
  # If only one "task" in rank_org, you get exactly 1 map => a single ggplot
  p_sig <- significanceMap(rank_org) +
    ggtitle(paste("Significance Map for", org, "(", toupper(metric_choice), ")"))
  
  ### Figure 3: a violin plot of rank distributions
  p_violin <- violin(ranking_boot) +
    coord_flip() +
    labs(title = paste("Violin of Ranks:", org, "(", metric_choice, ")"))
  
  # Collect them in a small list:
  organ_plots <- list(
    boxplot        = p_box,
    significance   = p_sig,
    violin_of_ranks= p_violin
  )
  
  # store in results
  results_per_organ[[org]] <- list(
    organ       = org,
    metric      = metric_choice,
    ranking_obj = rank_org,
    plots       = organ_plots
  )
  
  # 4) Save these 3 plots to files if you want
  # e.g. in a single PDF or 3 separate PNG
  ggsave(file.path(out_plots_dir, paste0("box_", org, "_", metric_choice, ".png")),
         p_box, width=7, height=5, dpi=300)
  ggsave(file.path(out_plots_dir, paste0("sigmap_", org, "_", metric_choice, ".png")),
         p_sig, width=6, height=6, dpi=300)
  ggsave(file.path(out_plots_dir, paste0("violin_", org, "_", metric_choice, ".png")),
         p_violin, width=7, height=5, dpi=300)
}

##############################################################################
## Done. You now have a loop that (1) picks DSC or NSD per organ, 
## (2) merges the right CSV from each algorithm, 
## (3) builds a ranking, and (4) outputs three relevant figures.
##############################################################################



### Load necessary library
library(dplyr)

# Assuming df is your original dataset

# Define the minimum count (528)
min_count <- 528

# Select the first 528 rows for each algorithm
df_balanced <- df_metric %>%
  group_by(algo) %>%
  slice_head(n = min_count) %>%
  ungroup()

# Save to CSV (optional)
# write.csv(df_balanced, "balanced_dataset.csv", row.names = FALSE)

df_balanced


chal_org <- as.challenge(
df_metric,
algorithm   = "algo",
case        = "case",
value       = "value",
smallBetter = FALSE,
check       = FALSE
)


rank_org <- aggregateThenRank(
  chal_org,
  FUN = mean,
  na.treat = "na.rm",
  ties.method = "min",
  alpha = NULL,
  p.adjust.method = NULL
)






library(challengeR)
library(dplyr)
library(tidyr)
library(readr)

parent_dir <- "./totalsegmentator_results/final_14dec2024_results/totalsegmentator_results - table 3 in paper"
model_dirs <- list.dirs(parent_dir, recursive = FALSE)

organs <- c("aorta","gall_bladder","kidney_left","kidney_right",
            "liver","pancreas","postcava","spleen","stomach")

df_big <- NULL

for (mdir in model_dirs) {
  csv_file <- file.path(mdir, "dsc.csv")
  if (!file.exists(csv_file)) {
    message("Skipping no dsc.csv in ", mdir)
    next
  }
  tmp_whole <- read_csv(csv_file, show_col_types = FALSE)

  # pivot: ( name, aorta, spleen, etc ) -> long format => (name, task, value)
  long_part <- tmp_whole %>%
    select(name, all_of(organs)) %>%
    pivot_longer(
      cols      = organs,
      names_to  = "task",    # "task" is your organ
      values_to = "value"
    ) %>%
    rename(case = name) %>%
    mutate(algo = basename(mdir))

  df_big <- bind_rows(df_big, long_part)
}

# Filter out NA
df_big <- df_big %>% filter(!is.na(value))

# Now df_big has columns: (case, task, value, algo)
head(df_big)



library(dplyr)
library(tidyr)
library(challengeR)
library(readr)
library(ggplot2)

############################
## 1) Read and assemble data
############################

# Read your metadata file
metadata <- read_csv("./totalsegmentator_results/final_14dec2024_results/totalsegmentator_results - table 3 in paper/totalsegmentator_metadata.csv", show_col_types = FALSE)
# It has columns like: (image_id, age, gender, institute, split, manufacturer, ...)

# Merge df_big (which has "case"=ImageID, "algo", "task"=organ, "value"=DSC, etc.)
# so that each row also has the manufacturer
df_big2 <- df_big %>%
  left_join(metadata, by = c("case" = "image_id"))

# Suppose we only need columns: (case, algo, task, value, manufacturer)
df_big2 <- df_big2 %>%
  select(case, algo, task, value, manufacturer)

###########################################
## 2) Convert to a challengeR "challenge" object,
##    treating each manufacturer as its own "task"
###########################################

# By default, challengeR calls them “tasks.” We will treat each unique manufacturer as a new “task”.
# 'algorithm' = "algo"
# 'case'      = "case"
# 'value'     = "value"
# 'by'        = "manufacturer"  (the subgroup)
# 'smallBetter=FALSE' means bigger DSC is better; set TRUE if smaller is better
#   but typically for DSC we want bigger = better => smallBetter=FALSE

ch_obj <- as.challenge(
  df_big2,
  algorithm   = "algo",
  case        = "case",
  value       = "value",
  by          = "manufacturer",  # each manufacturer is a separate "task"
  smallBetter = FALSE,           # DSC → bigger is better
  na.treat    = "na.rm",         # or you can supply a numeric fill or function
  check       = TRUE
)

########################################################
## 3) Rank the methods with test-based approach
##    (or whichever you prefer)
########################################################

# This does Wilcoxon signed-rank tests for every pair of algorithms, merges them,
# then sorts algorithms by significance wins. If you prefer aggregateThenRank(), etc., go for it.

ranking_obj <- ch_obj %>%
  testThenRank(
    alpha = 0.05,         # significance level
    p.adjust.method = "none", 
    smallBetter = FALSE,  # DSC => bigger is better
    ties.method = "min"   # or "average", "random"
  )

###########################
## 4) Bootstrap the ranking
###########################
set.seed(1)  # for reproducibility
ranking_obj_boot <- ranking_obj %>%
  bootstrap(nboot = 1000)  # e.g. 1000 samples

##############################################
## 5) Create the blob plots for ranking stability
##############################################
# Typically we do a "stability by tasks" or the single big “stability” plot.

# (A) "Stability across tasks" to see how ranks vary from one manufacturer to the next:
stability_plot <- stability(ranking_obj_boot)
stability_plot

# This lumps all tasks on the same figure, showing rank distributions per algorithm.

# (B) "Stability by task" => you get separate facets/panels for each manufacturer:
stab_by_task_plot <- stabilityByTask(ranking_obj_boot)
stab_by_task_plot

# Or you can do a simpler "blob plot by algorithm" if you want a separate approach.
# See: ?stabilityByAlgorithm.bootstrap.list

#######################################
## 6) Additional styling or manual tweaks
#######################################

# If you want to customize e.g. the color scale, the shape, the max bubble size, or
# add medians ± 2.5–97.5% lines in your own code, check out the challengeR docs:
# e.g. stability(ranking_obj_boot, ordering = yourPreferredAlgorithmOrder, probs = c(0.025, 0.975))

# Example adding custom shape or adjusting bubble scale:
stability(ranking_obj_boot, shape=4, max_size=6, probs=c(0.025, 0.975)) +
  theme_minimal()



